{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "### Due: Fri Dec 18th @ 11:59pm ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will covering NLP, Topic Modeling and Recommendation Engines\n",
    "\n",
    "We will generate recommendations on products from a department store based on product descriptions.\n",
    "We'll first transform the data into topics using Latent Dirichlet Allocation, and then generate recommendations based on this new representation using Content-Based Filtering.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- **Follow the comments below and fill in the blanks (____)**\n",
    "- **Please use default arguments whenever arguments aren't specified.**\n",
    "- **Please 'Restart and Run All' prior to submission.**\n",
    "- **When submitting to Gradescope, please mark on which page each question is answered.**\n",
    "\n",
    "\n",
    "Out of 28 points total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a recommendation engine for products from a department store.  \n",
    "The recommendations will be based on the similarity of product descriptions.  \n",
    "We'll use Content-Filtering to query a product and get back a list of products that are similar.  \n",
    "Instead of using the descriptions directly, we will first do some topic modeling using LDA to transform the descriptions into a topic space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   uniq_id        5000 non-null   object\n",
      " 1   sku            5000 non-null   object\n",
      " 2   name_title     5000 non-null   object\n",
      " 3   description    5000 non-null   object\n",
      " 4   category       4698 non-null   object\n",
      " 5   category_tree  4698 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# 1. (2pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product descriptions from the department store JCPenney.\n",
    "\n",
    "# Load product information from ../data/jcpenney-products_subset.csv.zip\n",
    "# This is a compressed version of a csv file.\n",
    "# Use pandas read_csv function with the default parameters.\n",
    "# read_csv has an argument 'compression' with default value 'infer' that will handle unzipping the data.\n",
    "# There is no need to unzip the data prior to using read_csv.\n",
    "# Store the resulting dataframe as df_products.\n",
    "df_products = pd.read_csv('../data/jcpenney-products_subset.csv.zip')\n",
    "\n",
    "# print a summary of df_products using .info. There should be 5000 records and 6 columns\n",
    "df_products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred Dunner® Essential Pull On Capri Pant\n",
      "---\n",
      "You'll return to our Alfred Dunner pull-on capris again and again when you want an updated, casual look and all the comfort you love.   elastic waistband approx. 19-21\" inseam slash pockets polyester washable imported      \n"
     ]
    }
   ],
   "source": [
    "# 2. (2pts) Print an Example\n",
    "\n",
    "# The two columns of the dataframe we're interested in are:\n",
    "#   'name_title' which is the name of the product stored as a string\n",
    "#   'description' which is a description of the product stored as a string\n",
    "#\n",
    "# We'll print out the product in the first row as an example\n",
    "# If we try to print both columns at the same time, pandas will truncate the strings\n",
    "#   so we'll print them seperately\n",
    "\n",
    "# print the column 'name_title' in row 0 of df_products\n",
    "print(df_products.name_title[0])\n",
    "\n",
    "print('---') # to visually separate the two strings\n",
    "\n",
    "# print the column 'desciption' in row 0 of df_products\n",
    "print(df_products.description[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (4pts) Transform Descriptions using TfIdf\n",
    "\n",
    "# In order to pass our product descriptions to the LDA model, we first need to vectorize \n",
    "#   from strings to fixed-length feature vectors.\n",
    "# To do this we will transform our documents using Tf-Idf,\n",
    "\n",
    "# Import TfidfVectorizer from sklearn.feature_extraction.text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#  Instantiate a TfidfVectorizer with\n",
    "#   ngram_range=(1,2), use both unigrams and bigrams\n",
    "#   min_df=20,         excluding terms which appear in less than 20 documents\n",
    "#   max_df=.7          excluding terms which appear in more than 70% of documents   \n",
    "#   all other arguments as their default\n",
    "\n",
    "# Store as tfidf\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,2),\n",
    "                       min_df = 20,\n",
    "                       max_df = .7)\n",
    "\n",
    "# fit_transform tfidf on the 'description' column of the df_products dataframe \n",
    "# Store the transformed dataset as X_tfidf\n",
    "X_tfidf = tfidf.fit_transform(df_products.description)\n",
    "\n",
    "# Assert that the shape of X_tfidf is 5000 rows by 2732 columns\n",
    "assert X_tfidf.shape == (5000, 2732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zip_front', 'zip_pocket', 'zipper', 'zipper_closure', 'zippered']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. (4pts) Format Bigrams and Print Sample of Extracted Vocabulary \n",
    "\n",
    "# The extracted vocabulary can be retrieved from tfidf as a list using .get_feature_names()\n",
    "# Store the extracted vocabulary list in the variable vocab\n",
    "vocab = tfidf.get_feature_names()\n",
    "\n",
    "# Sklearn joins bigrams with a space character.\n",
    "# To make output easier to read, replace all spaces in the vocab list with underscores.\n",
    "# To do this we can use the string .replace() method.\n",
    "# For example, if x is a string, x.replace(' ','_') will replace all ' ' with '_' in x.\n",
    "# A list comprehension would be useful here, but use any method you like\n",
    "#    to iterate through each item in vocab, replacing spaces with underscores.\n",
    "# Store the result back into vocab\n",
    "vocab = [i.replace(' ', '_') for i in vocab]\n",
    "\n",
    "# Print the last 5 terms in the vocabulary (should end with 'zippered')\n",
    "vocab[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. (4pts) Perform Topic Modeling with LDA\n",
    "\n",
    "# Now that we have our vectorized data, we can use Latent Direchlet Allocation to learn \n",
    "#   the per-document topic distributions and per-topic term distributions.\n",
    "# Though there are likely more, we'll model our dataset using 20 topics to keep things small.\n",
    "\n",
    "# Import LatentDirichletAllocation from sklearn.decomposition\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate a LatentDirichletAllocation model with\n",
    "#    n_components=20    fit 20 topics\n",
    "#    n_jobs=-1,         use all cores\n",
    "#    random_state=123   for reproducability\n",
    "# Store the model as lda\n",
    "lda = LatentDirichletAllocation(n_components= 20,\n",
    "                               n_jobs= -1,\n",
    "                               random_state= 123)\n",
    "\n",
    "# Run fit_transform() on lda using X_tfidf.\n",
    "# Store the output (the per-document topic distributions) as X_lda\n",
    "X_lda = lda.fit_transform(X_tfidf)\n",
    "\n",
    "# Assert that the shape of X_lda is 5000 rows by 20 columns\n",
    "assert X_lda.shape == (5000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0 : dress sleeveless length_from shoulder from_shoulder\n",
      "Topic  1 : elastic shorts cargo pockets waistband\n",
      "Topic  2 : pocket interior pockets exterior zip\n",
      "Topic  3 : spandex our front with inseam\n",
      "Topic  4 : fit sits leg straight waist\n",
      "Topic  5 : spot spot_clean clean_imported clean pillow\n",
      "Topic  6 : upper sole rubber synthetic rubber_sole\n",
      "Topic  7 : polyester_washable washable_imported washable regular drawstring\n",
      "Topic  8 : set safe king dishwasher comforter\n",
      "Topic  9 : the to for with of\n",
      "Topic 10 : button collar shirt button_front chest\n",
      "Topic 11 : spread spread_collar lycra dress_shirt stays\n",
      "Topic 12 : sleepwear safety flame flame_resistant meet\n",
      "Topic 13 : rug resistant yes indoor backing\n",
      "Topic 14 : rod sold sold_individually individually rod_pocket\n",
      "Topic 15 : clean_only dry_clean only_imported only tie\n",
      "Topic 16 : short sleeves cotton tee washable_imported\n",
      "Topic 17 : case dial color width show\n",
      "Topic 18 : metal jewelry photos show_detail photos_are\n",
      "Topic 19 : moisture wicking moisture_wicking socks dri\n"
     ]
    }
   ],
   "source": [
    "# 6. (5pts) Print Top Topic Terms\n",
    "\n",
    "# To get a sense of what each topic is composed of, we can print the most likely terms for each topic.\n",
    "# For each topic print 'Topic {topic_idx:2d} : ' followed by \n",
    "#   the top 5 most likely terms in that topic given the per-topic term distribution\n",
    "# Example:\n",
    "#    Topic  0 : wicking moisture moisture_wicking dri fabric\n",
    "\n",
    "# We'll use the vocab created above, but first convert from a list to np.array to make indexing easier\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "# This function returns a list of the top n terms in vocab given the weights in term_weights\n",
    "def get_top_terms(term_weights, top_n=5):\n",
    "    # np.argsort() returns the indices of an np.array sorted by their value, in ascending order\n",
    "    # [::-1] reverses the order of an np.array (descending order)\n",
    "    # list() converts from an np.array() back to a list\n",
    "    return list(vocab[np.argsort(term_weights)[::-1]][:top_n])\n",
    "\n",
    "# The per-topic term distributions (term_weights) are stored in lda.components_\n",
    "# For each array of term_weights in lda.components_ \n",
    "#    use get_top_terms() to print the top 5 terms per topic.\n",
    "# Example:\n",
    "#    Topic  0 : wicking moisture moisture_wicking dri fabric\n",
    "# Hints:\n",
    "#   use enumerate to get a topic_idx and the term_weights from lda.components_\n",
    "#   prefix each line with the string produced by f'Topic {topic_idx:2d} : '\n",
    "#   use ' '.join() to join the list of terms returned by get_top_terms()\n",
    "weights = lda.components_\n",
    "for topic_idx in range(len(weights)):\n",
    "    print (f'Topic {topic_idx:2d} :', ' '.join(get_top_terms(weights[topic_idx])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. (3pts) Generate Similarity Matrix\n",
    "\n",
    "# We'll use Content-Based Filtering to make recommendations based on a query product.\n",
    "# Each product will be represented by its LDA topic weights learned above.\n",
    "# We'd like to recommend products similar in LDA space.\n",
    "# We'll use euclidan distance as a measure of similarity.\n",
    "\n",
    "# Import euclidean_distances from sklearn.metrics.pairwise \n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Use euclidean_distances to generate similarity scores on our X_lda data\n",
    "# Recall that when using distance as similarity, smaller is better.\n",
    "# Store in a variable named distances.\n",
    "# NOTE: we only need to pass X_lda in once,\n",
    "#   the function will calculate pairwise distances for all rows in that matrix\n",
    "distances = euclidean_distances(X_lda)\n",
    "\n",
    "# Assert that the shape of the similarities matrix is 5000 rows by 5000 columns\n",
    "assert distances.shape == (5000,5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Alfred Dunner® Essential Pull On Capri Pant\n",
       "2481    City Streets® Colorblocked Performance Capris ...\n",
       "1351              Made for Life™ Pintucked Bermuda Shorts\n",
       "2671    Total Girl® French Terry Knit-Waist Capris - G...\n",
       "43         Dickies® Womens Youth Cargo Scrub Pants–Petite\n",
       "2985    Liz Claiborne® City-Fit Straight-Leg Jeans - Tall\n",
       "2559               Liz Claiborne® Sculpted Denim Jeggings\n",
       "4427                     Dockers® Cargo Shorts–Big & Tall\n",
       "3620                         Alfred Dunner® Pull-On Pants\n",
       "4619        Dickies® Women’s Youth Cargo Scrub Pants–Tall\n",
       "Name: name_title, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.(4pts) Generate Recommendations\n",
    "\n",
    "# Let's test our proposed recommendation engine using the product at row 0 in df_products as the query.\n",
    "#   The name of this product is \"Alfred Dunner® Essential Pull On Capri Pant\"\n",
    "\n",
    "# Print the names for the top 10 most similar products to this query.\n",
    "# Suggested way to do this is:\n",
    "#   get the euclidean distances from row 0 of the distances matrix\n",
    "#   get the indices of this array sorted by value using np.argsort()\n",
    "#   get the first 10 indexes from this array\n",
    "#   use those indices to index into df_products.name_title and print the result\n",
    "\n",
    "# NOTE: The first product should be:\n",
    "#   'Alfred Dunner® Essential Pull On Capri Pant', (the original query product)\n",
    "query_idx = 0\n",
    "best_idxs_asc = np.argsort(distances[query_idx])\n",
    "first_10idx= best_idxs_asc[:10]\n",
    "df_products.name_title[first_10idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f20",
   "language": "python",
   "name": "eods-f20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
